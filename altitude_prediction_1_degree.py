# -*- coding: utf-8 -*-
"""FODS_Assignment_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10H3RjmZKMlzThHRggg2NGxERPvOWZmBQ

##Data Preprocessing - Making DataFrame, Splitting data and Normalizing Values
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from tqdm import tqdm_notebook
import matplotlib.colors
from mpl_toolkits import mplot3d

training_data= pd.read_csv("3D_spatial_network.csv")

training_data.head()

training_data_new= training_data.drop('id',axis=1)

training_data_new.head()

train=training_data_new.sample(frac=0.7,random_state=1) #random state is a seed value
test=training_data_new.drop(train.index)

X_train = train.drop('altitude',axis=1)
Y_train = train['altitude']
X_test = test.drop('altitude',axis=1)
Y_test = test['altitude']

Y_train.head()

Y_train=pd.DataFrame(Y_train)
Y_test = pd.DataFrame(Y_test)

X_train.head()

Y_test.head()

X_train.describe()

X_train_normalize = (X_train-X_train.mean())/X_train.std()
X_train_normalize

training_X = X_train_normalize.values   ##numpy arrays

Y_train_normalize = (Y_train-Y_train.mean())/Y_train.std()
Y_train_normalize

training_Y = Y_train_normalize.values

X_test_normalize = (X_test-X_train.mean())/X_train.std()
X_test_normalize 
Y_test_normalize = (Y_test-Y_train.mean())/Y_train.std()
Y_test_normalize 
testing_Y = Y_test_normalize.values
testing_X = X_test_normalize.values

testing_X[0]

X_train_normalize.std()

"""##Testing with sklearn (Ideal Model)"""

reg = LinearRegression()
reg = reg.fit(training_X, training_Y)
Y_pred  = reg.predict(training_X)
rscore = reg.score(training_X,training_Y)
print(rscore)

"""##Main Code : Stochastic Gradient Descent"""

wt_matrix = []
Loss=[]
wt = np.random.randn(1,2)
b=0
epochs = 4
eta = 0.001
stopping_criteria = 0.001
print(wt)

for x in (range(0,epochs)):
  for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
    H = (np.dot(training_X,wt.T)) + b
    grad_w = (H[i] - training_Y[i])* training_X[i]
    grad_b = H[i] - training_Y[i]
    wt = wt - eta*(grad_w)
    b = b- eta*(grad_b)    
    wt_matrix.append(wt)
    Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) 
    print(Error)
  Loss.append(Error[0,0])
  if(Loss[i]-Loss[i-1]<stopping_criteria)  
  break

plt.plot(Loss,'*-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

Y_train_normalize.describe()

type(Y_train_normalize)

sums=0
for i in range(len(training_Y)):
  sq = training_Y[i]**2
  sums+=sq

sst = sums
sse = 148256
print(sst)

r_square = 1 - (sse/sst)
print(r_square)

"""##Main Code: Normal Gradient Descent"""

wt_matrix = []

wt = np.zeros((1,2))
b=0
epochs = 10    #@param {type: "slider", min: 1, max: 200}
eta = 0.000001  
stopping_criteria= 0.0001

Loss=[]
for x in (range(0,epochs)):
  grad_w=0
  grad_b=0
  H = (np.dot(training_X,wt.T)) + b
  for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
    grad_w += (H[i] - training_Y[i])* training_X[i]
    grad_b += H[i] - training_Y[i]
  wt = wt - eta*(grad_w)
  b = b- eta*(grad_b)    
  wt_matrix.append(wt)

  Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) 
  print(Error)
  Loss.append(Error[0,0])        ## Here actual error starts from 1st iteration. 
  # print(Loss[0], Loss[1])
  if (Loss[x-1]-Loss[x]<stopping_criteria and x!=0):
    print(x)
    print(Loss[x-1]-Loss[x])
    break                            ## So index of loss represents iteration number. 
                                ## 0th index has no meaning

print(((np.array((wt_matrix)))))

print(wt.shape)

plt.plot(Loss,'*-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

sums=0
for i in range(len(training_Y)):
  sq = training_Y[i]**2
  sums+=sq

sst = sums
sse = Error
print(sst)

r_square = 1 - (sse/sst)
print(r_square)

### Calculating error on Test Data ###
H_test = np.dot(testing_X, wt.T) + b
Error_Test = 0.5*(np.dot(((H_test - testing_Y).T),(H_test - testing_Y)))
sse_test = Error_Test[0][0]
print(sse_test)

sums=0
for i in range(len(testing_Y)):
  sq = (testing_Y[i])**2
  sums+=sq
sst_test = sums
r_square = 1 - (sse_test/sst_test)
print(r_square)

"""###3_D plot of Error wrt weights"""

type(training_X)

wt

w1 = np.linspace(-5,5,30)
w2 = np.linspace(-5,5,30)
# w1_c = 0.1482
# w2_c = -0.1893
ww1, ww2 = np.meshgrid(w1,w2)

#Y_pred = ww1 + ww2
# Y_pred = 
#E = 0.5*((training_Y-Y_pred)**2)
# Loss = np.zeros(WW.shape)
# print(Y_pred.shape)

E = np.zeros(ww1.shape)
for i in range(len(w1)):
  for j in range(len(w2)):
    w_new = []
    w_new.append(ww1[i,j])
    w_new.append(ww2[i,j])
    # loss=0
    # for i,j in zip(training_X,training_Y):
    #   loss+=(y-sigmoid(w_est,x,b_est))**2
    transposed_array=np.array((w_new)).reshape(-1,1)
    H = np.dot(training_X, transposed_array)
    Err = 0.5*(np.dot(((H - training_Y).T),(H - training_Y)))
    E[i][j] = (Err[0][0])

np.dot(training_X,transposed_array).shape

my_cmap= matplotlib.colors.LinearSegmentedColormap.from_list("",["red","yellow","green"])

# To plot the 3-d Graph on a surface as a graded output

plt.contourf(ww1,ww2,E,cmap=my_cmap,alpha=.6)
plt.show()

fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_surface(ww1, ww2, E, cmap='viridis')
ax.set_xlabel('W1')
ax.set_ylabel('W2')
ax.set_zlabel('Error')

ax.view_init(30, 270)
fig

"""##Main Code: Normal Equations Method"""

##Adding a column of 1's in training_X
normal_X = X_train_normalize
normal_Y = Y_train_normalize
normal_X['constant'] = 1
normal_X.head()

dot_product = np.dot((normal_X.T),normal_X)
normal_inverse = np.linalg.inv(dot_product)
temp = np.dot(normal_X.T, normal_Y) 
normal_wt = np.dot(normal_inverse, temp)
print(normal_wt)

print(normal_wt.shape)

H = np.dot(normal_X, normal_wt)
normal_error = 0.5*(np.dot(((H - normal_Y).T),(H - normal_Y)))
print(normal_error)

sse = normal_error[0,0]
r_square = 1 - (sse/sst)
print(r_square)

"""## Gradient Descent with L2-Regularization"""

wt_matrix = []
Loss=[]
wt = np.zeros((1,2))
b=0
alpha = 0.0004
epochs = 20
eta = 0.000001
print(wt)

for x in (range(0,epochs)):
  grad_w=0
  grad_b=0
  H = (np.dot(training_X,wt.T)) + b
  for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
    grad_w += (H[i] - training_Y[i])* training_X[i] + alpha *(wt)
    grad_b += H[i] - training_Y[i] + alpha*b
  wt = wt - eta*(grad_w)
  b = b- eta*(grad_b)    
  wt_matrix.append(wt)
  Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) + 0.5*alpha*(np.dot(wt,wt.T)) 
  print(Error)
  Loss.append(Error[0,0])        ## Here actual error starts from 1st iteration. 
                                 ## So index of loss represents iteration number. 
                                ## 0th index has no meaning

plt.plot(Loss,'*-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

##Plotting alpha vs error function
alpha_array = np.linspace(0.0004,0.004,10)
Loss = []
print(epochs)

for alpha in alpha_array:
  for x in (range(0,epochs)):
    grad_w=0
    grad_b=0
    H = (np.dot(training_X,wt.T)) + b
    for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
      grad_w += (H[i] - training_Y[i])* training_X[i] + alpha *(wt)
      grad_b += H[i] - training_Y[i] + alpha*b
    wt = wt - eta*(grad_w)
    b = b- eta*(grad_b)    
    wt_matrix.append(wt)
  Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) + 0.5*alpha*(np.dot(wt,wt.T)) 
  print(Error)
  Loss.append(Error[0,0])

plt.plot(((alpha_array)),Loss,'*-')
plt.show()

"""## Gradient Descent with L1-Regularization"""

wt_matrix = []
Loss=[]
wt = np.zeros((1,2))
b=0
alpha = 0.0004
epochs = 15
eta = 0.000001
print(wt)

for x in (range(0,epochs)):
  grad_w=0
  grad_b=0
  H = (np.dot(training_X,wt.T)) + b
  for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
    grad_w += (H[i] - training_Y[i])* training_X[i] + 0.5*alpha *(np.sign(wt))
    grad_b += H[i] - training_Y[i] + 0.5*alpha*(np.sign(b))
  wt = wt - eta*(grad_w)
  b = b- eta*(grad_b)    
  wt_matrix.append(wt)
  Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) + 0.5*alpha*(np.sum(np.absolute(wt))) 
  print(Error)
  Loss.append(Error[0,0])        ## Here actual error starts from 1st iteration. 
                                 ## So index of loss represents iteration number. 
                                ## 0th index has no meaning

plt.plot(Loss,'*-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

##Plotting alpha vs error function
alpha_array = np.linspace(0.0002,0.0008,20)

for alpha in alpha_array:
  for x in (range(0,epochs)):
    grad_w=0
    grad_b=0
    H = (np.dot(training_X,wt.T)) + b
    for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
      grad_w += (H[i] - training_Y[i])* training_X[i] + 0.5*alpha *(np.sign(wt))
      grad_b += H[i] - training_Y[i] + 0.5*alpha*(np.sign(b))
    wt = wt - eta*(grad_w)
    b = b- eta*(grad_b)    
    wt_matrix.append(wt)
Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) + 0.5*alpha*(np.sum(np.absolute(wt))) 
print(Error)
Loss.append(Error[0,0])        ## Here actual error starts from 1st iteration. 
                                 ## So index of loss represents iteration number. 
                                ## 0th index has no meaning

plt.plot(Loss,alpha_array,'*-')
plt.show()