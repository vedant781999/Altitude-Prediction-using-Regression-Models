# -*- coding: utf-8 -*-
"""FODS_6_degree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VCHtMo91I4CgDYAMQbf3SrqDoEN4rs1S

#Data Preprocessing - Making DataFrame, Splitting data and Normalizing Values
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from tqdm import tqdm_notebook
import matplotlib.colors
from mpl_toolkits import mplot3d
from math import sqrt

from google.colab import drive
drive.mount('/content/drive/')
root_path = '/content/drive/My Drive/3D_spatial_network.csv'

training_data= pd.read_csv(root_path)

training_data.head()

training_data_new= training_data.drop('id',axis=1)

training_data_new.head()

train=training_data_new.sample(frac=0.7,random_state=1) #random state is a seed value
test=training_data_new.drop(train.index)

X_train = train.drop('altitude',axis=1)
Y_train = train['altitude']
X_test = test.drop('altitude',axis=1)
Y_test = test['altitude']

Y_train.head()

Y_train=pd.DataFrame(Y_train)
Y_test = pd.DataFrame(Y_test)

X_train.head()

Y_test.head()

X_train.describe()

X_train_normalize = (X_train-X_train.mean())/X_train.std()
X_train_normalize

training_X = X_train_normalize.values   ##numpy arrays

Y_train_normalize = (Y_train-Y_train.mean())/Y_train.std()
Y_train_normalize

training_Y = Y_train_normalize.values

X_test_normalize = (X_test-X_train.mean())/X_train.std()
X_test_normalize 
Y_test_normalize = (Y_test-Y_train.mean())/Y_train.std()
Y_test_normalize 
testing_Y = Y_test_normalize.values
testing_X = X_test_normalize.values

testing_X[0]

X_train_normalize.std()
training_data_new = X_train_normalize
testing_data_new = X_test_normalize

x1_list = training_data_new['latitude'].tolist()
x1_square_list = list(map(lambda x: x**2 , x1_list))
training_data_new['x1_square'] = x1_square_list

x2_list = training_data_new['longitude'].tolist()
x2_square_list = list(map(lambda x: x**2 , x2_list))
training_data_new['x2_square'] = x2_square_list

x1_list = training_data_new['latitude'].tolist()
x2_list = training_data_new['longitude'].tolist()
x1_x2_list = list(map(lambda x,y: x*y , x1_list, x2_list))
training_data_new['x1_x2'] = x1_x2_list

x1_list = training_data_new['latitude'].tolist()
x1_cube_list = list(map(lambda x: x**3 , x1_list))
training_data_new['x1_cube'] = x1_cube_list

x2_list = training_data_new['longitude'].tolist()
x2_cube_list = list(map(lambda x: x**3 , x2_list))
training_data_new['x2_cube'] = x2_cube_list

x1_square_x2_list = list(map(lambda x,y: x*y , x1_square_list, x2_list))
training_data_new['x1_square_x2'] = x1_square_x2_list

x2_square_x1_list = list(map(lambda x,y: x*y , x2_square_list, x1_list))
training_data_new['x2_square_x1'] = x2_square_x1_list

x1_list = training_data_new['latitude'].tolist()
x1_power_4_list = list(map(lambda x: x**4 , x1_list))
training_data_new['x1_power_4'] = x1_power_4_list

x2_list = training_data_new['longitude'].tolist()
x2_power_4_list = list(map(lambda x: x**4 , x2_list))
training_data_new['x2_power_4'] = x2_power_4_list

x1_cube_x2_list = list(map(lambda x,y: x*y , x1_cube_list, x2_list))
training_data_new['x1_cube_x2'] = x1_cube_x2_list

x2_cube_x1_list = list(map(lambda x,y: x*y , x1_list, x2_cube_list))
training_data_new['x2_cube_x1'] = x2_cube_x1_list

x1_square_x2_square_list = list(map(lambda x,y: x*y , x1_square_list, x2_square_list))
training_data_new['x1_square_x2_square'] = x1_square_x2_square_list

x1_power_5_list = list(map(lambda x: x**5 , x1_list))
training_data_new['x1_power_5'] = x1_power_5_list

x2_power_5_list = list(map(lambda x: x**5 , x2_list))
training_data_new['x2_power_5'] = x2_power_5_list

x1_power_4_x2_list = list(map(lambda x,y: x*y , x1_power_4_list, x2_list))
training_data_new['x1_power_4_x2'] = x1_power_4_x2_list

x2_power_4_x1_list = list(map(lambda x,y: x*y , x2_power_4_list, x1_list))
training_data_new['x2_power_4_x1'] = x2_power_4_x1_list

x1_cube_x2_square_list = list(map(lambda x,y: x*y , x1_cube_list, x2_square_list))
training_data_new['x1_cube_x2_square'] = x1_cube_x2_square_list

x1_square_x2_cube_list = list(map(lambda x,y: x*y , x1_square_list, x2_cube_list))
training_data_new['x1_square_x2_cube'] = x1_square_x2_cube_list

x1_power_6_list = list(map(lambda x: x**6 , x1_list))
training_data_new['x1_power_6'] = x1_power_6_list

x2_power_6_list = list(map(lambda x: x**6 , x2_list))
training_data_new['x2_power_6'] = x2_power_6_list

x1_power_5_x2_list = list(map(lambda x,y: x*y , x1_power_5_list, x2_list))
training_data_new['x1_power_5_x2'] = x1_power_5_x2_list

x2_power_5_x1_list = list(map(lambda x,y: x*y , x2_power_5_list, x1_list))
training_data_new['x2_power_5_x1'] = x2_power_5_x1_list

x1_power_4_x2_square_list = list(map(lambda x,y: x*y , x1_power_4_list, x2_square_list))
training_data_new['x1_power__4_x2_square'] = x1_power_4_x2_square_list

x1_square_x2_power_4_list = list(map(lambda x,y: x*y , x1_square_list, x2_power_4_list))
training_data_new['x1_square_x2_power_4_cube'] = x1_square_x2_power_4_list

x1_cube_x2_cube_list = list(map(lambda x,y: x*y , x1_cube_list, x2_cube_list))
training_data_new['x1_cube_x2_cube'] = x1_cube_x2_cube_list

##Testing_data

x1_list = testing_data_new['latitude'].tolist()
x1_square_list = list(map(lambda x: x**2 , x1_list))
testing_data_new['x1_square'] = x1_square_list

x2_list = testing_data_new['longitude'].tolist()
x2_square_list = list(map(lambda x: x**2 , x2_list))
testing_data_new['x2_square'] = x2_square_list

x1_list = testing_data_new['latitude'].tolist()
x2_list = testing_data_new['longitude'].tolist()
x1_x2_list = list(map(lambda x,y: x*y , x1_list, x2_list))
testing_data_new['x1_x2'] = x1_x2_list

x1_list = testing_data_new['latitude'].tolist()
x1_cube_list = list(map(lambda x: x**3 , x1_list))
testing_data_new['x1_cube'] = x1_cube_list

x2_list = testing_data_new['longitude'].tolist()
x2_cube_list = list(map(lambda x: x**3 , x2_list))
testing_data_new['x2_cube'] = x2_cube_list

x1_square_x2_list = list(map(lambda x,y: x*y , x1_square_list, x2_list))
testing_data_new['x1_square_x2'] = x1_square_x2_list

x2_square_x1_list = list(map(lambda x,y: x*y , x2_square_list, x1_list))
testing_data_new['x2_square_x1'] = x2_square_x1_list

x1_list = testing_data_new['latitude'].tolist()
x1_power_4_list = list(map(lambda x: x**4 , x1_list))
testing_data_new['x1_power_4'] = x1_power_4_list

x2_list = testing_data_new['longitude'].tolist()
x2_power_4_list = list(map(lambda x: x**4 , x2_list))
testing_data_new['x2_power_4'] = x2_power_4_list

x1_cube_x2_list = list(map(lambda x,y: x*y , x1_cube_list, x2_list))
testing_data_new['x1_cube_x2'] = x1_cube_x2_list

x2_cube_x1_list = list(map(lambda x,y: x*y , x1_list, x2_cube_list))
testing_data_new['x2_cube_x1'] = x2_cube_x1_list

x1_square_x2_square_list = list(map(lambda x,y: x*y , x1_square_list, x2_square_list))
testing_data_new['x1_square_x2_square'] = x1_square_x2_square_list

x1_power_5_list = list(map(lambda x: x**5 , x1_list))
testing_data_new['x1_power_5'] = x1_power_5_list

x2_power_5_list = list(map(lambda x: x**5 , x2_list))
testing_data_new['x2_power_5'] = x2_power_5_list

x1_power_4_x2_list = list(map(lambda x,y: x*y , x1_power_4_list, x2_list))
testing_data_new['x1_power_4_x2'] = x1_power_4_x2_list

x2_power_4_x1_list = list(map(lambda x,y: x*y , x2_power_4_list, x1_list))
testing_data_new['x2_power_4_x1'] = x2_power_4_x1_list

x1_cube_x2_square_list = list(map(lambda x,y: x*y , x1_cube_list, x2_square_list))
testing_data_new['x1_cube_x2_square'] = x1_cube_x2_square_list

x1_square_x2_cube_list = list(map(lambda x,y: x*y , x1_square_list, x2_cube_list))
testing_data_new['x1_square_x2_cube'] = x1_square_x2_cube_list

x1_power_6_list = list(map(lambda x: x**6 , x1_list))
testing_data_new['x1_power_6'] = x1_power_6_list

x2_power_6_list = list(map(lambda x: x**6 , x2_list))
testing_data_new['x2_power_6'] = x2_power_6_list

x1_power_5_x2_list = list(map(lambda x,y: x*y , x1_power_5_list, x2_list))
testing_data_new['x1_power_5_x2'] = x1_power_5_x2_list

x2_power_5_x1_list = list(map(lambda x,y: x*y , x2_power_5_list, x1_list))
testing_data_new['x2_power_5_x1'] = x2_power_5_x1_list

x1_power_4_x2_square_list = list(map(lambda x,y: x*y , x1_power_4_list, x2_square_list))
testing_data_new['x1_power__4_x2_square'] = x1_power_4_x2_square_list

x1_square_x2_power_4_list = list(map(lambda x,y: x*y , x1_square_list, x2_power_4_list))
testing_data_new['x1_square_x2_power_4_cube'] = x1_square_x2_power_4_list

x1_cube_x2_cube_list = list(map(lambda x,y: x*y , x1_cube_list, x2_cube_list))
testing_data_new['x1_cube_x2_cube'] = x1_cube_x2_cube_list

testing_data_new.head()

X_train_normalize = training_data_new
X_test_normalize = testing_data_new

training_X = X_train_normalize.values   ##numpy arrays
training_Y = Y_train_normalize.values

testing_Y = Y_test_normalize.values
testing_X = X_test_normalize.values


"""#L1 -Regularization"""

training_X.shape

wt_matrix = []
Loss=[]
wt = np.zeros((1,27))
b=0
stopping_criteria = 0.0001
count=0

alpha = 1e-16
epochs = 2000
eta = 1e-9
stopping_criteria = 0.0001
for x in (range(0,epochs)):
  grad_w=0
  grad_b=0
  count+=1
  H = (np.dot(training_X,wt.T)) + b
  for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
    grad_w += (H[i] - training_Y[i])* training_X[i] + 0.5*alpha *(np.sign(wt))
    grad_b += H[i] - training_Y[i] + 0.5*alpha*(np.sign(b))
  wt = wt - eta*(grad_w)
  b = b- eta*(grad_b)    
  # wt_matrix.append(wt)
  Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) + 0.5*alpha*(np.sum(np.absolute(wt))) 
  print(Error,count)
  Loss.append(Error[0,0])        ## Here actual error starts from 1st iteration. 
  if (abs(Loss[x-1]-Loss[x])<stopping_criteria and x!=0):
    print(x)
    print(Loss[x-1]-Loss[x])
    break                                        ## So index of loss represents iteration number.   
                        ## 0th index has no meaning

plt.plot(Loss,'*-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

##Plotting alpha vs error function
alpha_array = np.linspace(0,100,100)
epochs=20
count=0
Loss=[]
eta = 1e-9

for alpha in alpha_array:
  wt = np.zeros((1,27))
  b=0
  for x in (range(0,epochs)):
    count+=1
    grad_w=0
    grad_b=0
    H = (np.dot(training_X,wt.T)) + b
    for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
      grad_w += (H[i] - training_Y[i])* training_X[i] + 0.5*alpha *(np.sign(wt))
      grad_b += H[i] - training_Y[i] + 0.5*alpha*(np.sign(b))
    wt = wt - eta*(grad_w)
    b = b- eta*(grad_b)    
    #wt_matrix.append(wt)
    #print(count)
  Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) + 0.5*alpha*(np.sum(np.absolute(wt))) 
  print(Error,count)
  Loss.append(Error[0,0])        ## Here actual error starts from 1st iteration. 
                                 ## So index of loss represents iteration number. 
                                ## 0th index has no meaning

plt.plot(alpha_array,Loss,'*-')
plt.show()

sums=0
for i in range(len(training_Y)):
  sq = training_Y[i]**2
  sums+=sq
sst= sums

sse = (np.dot(((H - training_Y).T),(H - training_Y)))

rmse = sqrt(2*Error/training_Y.shape[0]) 
print(rmse,Error, sse)

r_square = 1 - (sse/sst)
print(r_square)

### Calculating error on Test Data ###
H_test = np.dot(testing_X, wt.T) + b
Error_Test = 0.5*(np.dot(((H_test - testing_Y).T),(H_test - testing_Y))) + 0.5*alpha*(np.sum(np.absolute(wt)))
sse_test = (np.dot(((H_test - testing_Y).T),(H_test - testing_Y)))
print(sse_test)

sums=0
for i in range(len(testing_Y)):
  sq = (testing_Y[i])**2
  sums+=sq
sst_test = sums
r_square = 1 - (sse_test/sst_test)
print(r_square)

rmse_test = sqrt(2*Error_Test/testing_Y.shape[0]) 
print(rmse_test,sse_test, Error_Test)

"""##L2 - Regularization"""

wt_matrix = []
Loss=[]
wt = np.zeros((1,27))
b=0
alpha = 1e-16
eta = 1e-9
count=0

alpha = 0
epochs = 500
eta = 1e-9
for x in (range(0,epochs)):
  grad_w=0
  grad_b=0
  count+=1
  H = (np.dot(training_X,wt.T)) + b
  for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
    grad_w += (H[i] - training_Y[i])* training_X[i] + alpha *(wt)
    grad_b += H[i] - training_Y[i] + alpha*b
  wt = wt - eta*(grad_w)
  b = b- eta*(grad_b)    
  Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) + 0.5*alpha*(np.dot(wt,wt.T)) 
  print(Error,count)
  Loss.append(Error[0,0])        ## Here actual error starts from 1st iteration. 
                                 ## So index of loss represents iteration number. 
                                ## 0th index has no meaning
  # if (abs(Loss[x-1]-Loss[x])<stopping_criteria):
  # # print(x)
  # print(Loss[x-1]-Loss[x])
  # break

plt.plot(Loss,'*-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

print(wt,b)

##Plotting alpha vs error function
alpha_array = np.linspace(0,100,100)
Loss = []
print(alpha_array)
epochs=20
eta = 1e-9
count=0

for alpha in alpha_array:
  print(alpha)
  wt = np.zeros((1,27))
  b=0
  for x in (range(0,epochs)):
    count+=1
    grad_w=0
    grad_b=0
    H = (np.dot(training_X,wt.T)) + b
    for i in tqdm_notebook(range(0,len(training_X)),total = len(training_X), unit = 'training_X'):
      grad_w += (H[i] - training_Y[i])* training_X[i] + alpha *(wt)
      grad_b += H[i] - training_Y[i] + alpha*b
    wt = wt - eta*(grad_w)
    b = b- eta*(grad_b)    
    # wt_matrix.append(wt)
  Error = 0.5*(np.dot(((H - training_Y).T),(H - training_Y))) + 0.5*alpha*(np.dot(wt,wt.T)) 
  print(Error,count)
  Loss.append(Error[0,0])        ## Here actual error starts from 1st iteration. 
                                  ## So index of loss represents iteration number. 
                                  ## 0th index has no meaning

print((Loss))

plt.plot(((alpha_array)),Loss,'*-')
plt.show()

print(alpha_array[0],alpha_array[1],Loss[0],Loss[1])

sums=0
for i in range(len(training_Y)):
  sq = training_Y[i]**2
  sums+=sq
sst= sums

sse = (np.dot(((H - training_Y).T),(H - training_Y)))

rmse = sqrt(2*Error/training_Y.shape[0]) 
print(rmse)

r_square = 1 - (sse/sst)
print(r_square)

### Calculating error on Test Data ###
H_test = np.dot(testing_X, wt.T) + b
Error_Test = 0.5*(np.dot(((H_test - testing_Y).T),(H_test - testing_Y))) + 0.5*alpha*(np.dot(wt,wt.T))
sse_test = (np.dot(((H_test - testing_Y).T),(H_test - testing_Y)))
print(sse_test)

sums=0
for i in range(len(testing_Y)):
  sq = (testing_Y[i])**2
  sums+=sq
sst_test = sums
r_square = 1 - (sse_test/sst_test)
print(r_square)

rmse_test = sqrt(2*Error_Test/testing_Y.shape[0]) 
print(rmse_test)